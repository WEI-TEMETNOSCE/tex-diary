<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>2025-09-20-machine-learning</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="post.css" />
  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
<p>&lt;TAGs&gt;: 2025, machine-learning, AI, algorithms</p>
<p><a href="run:2025-09-20-machine-learning.tex">September 20</a></p>
<h1 id="machine-learning-fundamentals-review">Machine Learning
Fundamentals Review</h1>
<p>Today I reviewed key machine learning concepts and algorithms. The
main focus was on understanding the mathematical foundations.</p>
<h2 id="supervised-learning">Supervised Learning</h2>
<ul>
<li><p><strong>Linear Regression</strong>: Minimizes the cost function
<span class="math inline">\(J(\theta) = \frac{1}{2m} \sum_{i=1}^{m}
(h_\theta(x^{(i)}) - y^{(i)})^2\)</span></p></li>
<li><p><strong>Logistic Regression</strong>: Uses sigmoid function <span
class="math inline">\(h_\theta(x) = \frac{1}{1 + e^{-\theta^T
x}}\)</span></p></li>
<li><p><strong>Support Vector Machines</strong>: Finds optimal
hyperplane with maximum margin</p></li>
</ul>
<h2 id="key-insights">Key Insights</h2>
<p>The bias-variance tradeoff is fundamental to understanding model
performance. High bias leads to underfitting, while high variance causes
overfitting.</p>
<p><strong>Regularization techniques:</strong></p>
<ol>
<li><p>L1 regularization (Lasso): <span class="math inline">\(\lambda
\sum_{j=1}^{n} |\theta_j|\)</span></p></li>
<li><p>L2 regularization (Ridge): <span class="math inline">\(\lambda
\sum_{j=1}^{n} \theta_j^2\)</span></p></li>
<li><p>Elastic Net: Combines L1 and L2</p></li>
</ol>
<h2 id="next-steps">Next Steps</h2>
<ul>
<li><p>Implement gradient descent algorithm</p></li>
<li><p>Study cross-validation techniques</p></li>
<li><p>Explore ensemble methods</p></li>
</ul>
</body>
</html>
