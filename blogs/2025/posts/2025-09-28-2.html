<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
    <meta charset="utf-8" />
    <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
    <title>2025 09 28 2</title>
    <link rel="stylesheet" href="post.css" />
    
    <!-- MathJax Configuration -->
    <script>
    window.MathJax = {
        tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            processEscapes: true,
            processEnvironments: true,
            macros: {
                "coloneq": "\\mathrel{\\mathop:}=",
                "RR": "\\mathbb{R}",
                "CC": "\\mathbb{C}",
                "NN": "\\mathbb{N}",
                "PP": "\\mathbb{P}",
                "E": "\\mathbb{E}",
                "D": "\\mathbb{D}",
                "P": "\\mathcal{P}",
                "H": "\\mathcal{H}",
                "X": "\\mathcal{X}",
                "B": "\\mathcal{B}",
                "F": "\\mathcal{F}",
                "M": "\\mathbb{M}",
                "var": "\\mathrm{var}",
                "Var": "\\mathrm{Var}",
                "cov": "\\mathrm{cov}",
                "Cov": "\\mathrm{Cov}",
                "corr": "\\mathrm{corr}",
                "Corr": "\\mathrm{Corr}",
                "pr": "\\mathrm{Pr}",
                "prob": "\\mathrm{Pr}",
                "normal": "\\mathcal{N}",
                "MSE": "\\mathrm{MSE}",
                "KL": "\\mathrm{KL}",
                "V": "\\mathbf",
                "vx": "\\boldsymbol{x}",
                "trace": "\\mathrm{trace}",
                "diag": "\\mathrm{diag}",
                "tt": "^\\top",
                "df": "\\mathrm{d}",
                "d": "\\mathrm{d}",
                "dt": "\\mathrm{d}t",
                "dW": "\\mathrm{d}W",
                "dno": "\\mathrm{d}",
                "dd": "\\nabla",
                "tdd": "\\tilde{\\nabla}",
                "ddt": "\\frac{\\mathrm{d}}{\\mathrm{d} t}",
                "div": "\\nabla\\cdot",
                "ind": "\\mathbb{I}",
                "sign": "\\mathrm{sign}",
                "cd": "\\mid",
                "la": "\\langle",
                "ra": "\\rangle",
                "sumstein": "\\mathcal{T}",
                "stein": "\\mathcal{A}",
                "score": "\\boldsymbol{s}",
                "ff": "\\boldsymbol{\\phi}",
                "eng": "E",
                "hattheta": "\\hat{\\theta}",
                "cc": "\\theta",
                "kernel": "\\mathbf{k}",
                "datai": "^{(i)}",
                "defeq": "\\mathrel{\\aban@defeq}",
                "workingDate": "\\textsc{<YEAR> $|$ <MONTH_NAME> $|$ <DAY>}",
                "userName": "<AUTHOR>",
                "institution": "<INSTITUTION>",
                "diaryTitle": "<DIARY_TITLE>",
                "myb": ["\\mbox{\\boldmath$#1$}", 1],
                "eqnref": ["Eqn.~\\eqref{#1}", 1],
                "figref": ["Fig.~\\ref{#1}", 1],
                "here": ["\\url{#1}", 1],
                "Qiang": ["\\textcolor{red}{#1}\\\\", 1],
                "qiang": ["\\textcolor{red}{#1}", 1],
                "Todo": ["\\textcolor{red}{TODO: #1}\\\\", 1],
                "todo": ["\\textcolor{gray}{TODO: #1}", 1],
                "remark": ["\\paragraph{Remark}#1", 1],
                "red": ["\\textcolor{red}{#1}", 1],
                "med": ["\\textcolor{magenta}{#1}", 1],
                "blue": ["\\textcolor{blue}{#1}", 1],
                "gray": ["\\textcolor{gray}{#1}", 1],
                "green": ["\\textcolor{green}{#1}", 1],
                "myempty": ["", 1],
                "vv": ["\\boldsymbol{#1}", 1],
                "neib": ["\\partial{#1}", 1],
                "set": ["\\mathcal{#1}", 1],
                "funcset": ["\\mathcal{#1}", 1],
                "opt": ["\\mathcal{#1}", 1],
                "greedy": ["\\mathrm{Greedy}\\{#1\\}", 1],
                "clickhere": ["\\href{run:#1}{Click here}", 1],
                "e": ["\\mathbb{E}\\left[#1\\right]", 1],
                "showlabels": ["", 1],
                "myf": ["\\frac{\\displaystyle #1}{\\displaystyle #2}", 2],
                "myp": ["\\frac{\\displaystyle \\partial #1}{\\displaystyle \\partial  #2}", 2],
                "f": ["\\frac{#1}{#2}", 2],
                "definecolorbox": ["%
  \\expandafter\\declaretheoremstyle\\expandafter[%
    headfont=\\bfseries\\sffamily\\color{#1!70!black}, 
    bodyfont=\\normalfont,
    mdframed={%
      linewidth=2pt,
      rightline=false, 
      topline=false, 
      bottomline=false,
      linecolor=#1, 
      backgroundcolor=#1!5,
    }
  ]{#2}%
", 2]
            }
        },
        options: {
            ignoreHtmlClass: "tex2jax_ignore",
            processHtmlClass: "tex2jax_process"
        }
    };
    </script>
    <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
</head>
<body>
    <p><a href="run:2025-09-28-2.tex">September 28</a></p>
<h1 id="intro-proximal-policy-optimization">Intro: Proximal Policy
Optimization</h1>
<h4 id="proximal-point-method">Proximal Point Method</h4>
<p>Going beyond gradient descent, proximal point methods offer a more
general framework for optimization. Starting from an initialization
<span class="math inline">\(\theta^0\)</span>, they perform iterative
updates of the form: <span class="math display">\[\theta^{k+1} =
\operatorname{arg\,max}_{\theta}
  \left \{ R(\theta) - \beta D(\theta, \theta^k) \right \},\]</span>
where, at iteration <span class="math inline">\(k\)</span>, the next
point <span class="math inline">\(\theta^{k+1}\)</span> is obtained by
maximizing the objective while penalizing deviation from <span
class="math inline">\(\theta^k\)</span>. Here, <span
class="math inline">\(D(\theta, \theta&#39;)\)</span> is a discrepancy
or distance measure between <span class="math inline">\(\theta\)</span>
and <span class="math inline">\(\theta&#39;\)</span>. The default choice
is <span class="math inline">\(D(\theta, \theta&#39;) =
\frac{1}{2}\norm{\theta - \theta&#39;}^2\)</span>, though it is common
to use KL divergence in policy optimization as we show below.</p>
<p>Each update from <span class="math inline">\(\theta^k\)</span> to
<span class="math inline">\(\theta^{k+1}\)</span> requires solving an
optimization problem, making the proximal point method a double-loop
algorithm. Since this inner problem is often intractable, it is
typically approximated using practical methods such as gradient descent
or proximal gradient descent.</p>
<p>Different approximations lead to different algorithmic variants.</p>
<h4 id="proximal-gradient-descent">Proximal Gradient Descent</h4>
<p>Applying a first-order Taylor approximation to the objective function
<span class="math inline">\(R(\theta)\)</span> gives: <span
class="math display">\[\theta^{k+1} = \operatorname{arg\,max}_{\theta}
  \left \{ \nabla R(\theta^k) ^\top (\theta - \theta^k) - \beta D(\theta,
\theta^k) \right \}.\]</span> This yields the <em>proximal gradient
descent</em> method.</p>
<h4 id="preconditioned-gradient-descent">Preconditioned Gradient
Descent</h4>
<p>Consider a quadratic proximal penalty: <span
class="math display">\[D(\theta, \theta^k) =
\frac{1}{2}(\theta-\theta^k)^\top J(\theta^k)(\theta - \theta^k),\]</span>
where <span class="math inline">\(J(\theta^k)\)</span> is a positive
definite preconditioning matrix chosen by the user. In this case,
proximal gradient descent admits a closed-form solution: <span
class="math display">\[\theta^{k+1} = \theta^k + J(\theta^k)^{-1} \nabla
R(\theta^k).\]</span> This is known as <em>preconditioned gradient
descent</em>.</p>
<div class="remark">
<p>Assume we use gradient descent to solve the inner loops of proximal
methods. The difference from vanilla gradient descent then lies in how
the <em>reference point</em> is updated. To see this, let us consider
the following generic update rule: <span
class="math display">\[\theta^{k+1}
= \operatorname{arg\,max}_\theta \Bigl\{ \nabla R(\theta^k)^\top (\theta - \theta^k)
- \beta D(\theta, \theta^{\mathrm{ref},k}) \Bigr\},\]</span> where <span
class="math inline">\(\theta^{\mathrm{ref},k}\)</span> is the reference
point used to anchor the update at iteration <span
class="math inline">\(k\)</span>. The choice of <span
class="math inline">\(\theta^{\mathrm{ref},k}\)</span> governs the
behavior of the algorithm:</p>
<ul>
<li><p>If <span class="math inline">\(\theta^{\mathrm{ref},k} =
\theta^k\)</span>, the update reduces to <em>proximal gradient
descent</em>, where the reference point is updated as fast as the
iterate itself.</p></li>
<li><p>If the reference point is updated only once every <span
class="math inline">\(m\)</span> iterations, i.e.Â <span
class="math inline">\(\theta^{\mathrm{ref},k} = \theta^{m\lfloor k/m
\rfloor}\)</span>, then each outer iteration corresponds to running
<span class="math inline">\(m\)</span> steps of a proximal gradient
algorithm to approximately solve the proximal point update.</p></li>
<li><p>If <span class="math inline">\(\theta^{\mathrm{ref},k}\)</span>
evolves more slowly, for example as an exponential moving average of
past iterates, the algorithm takes more conservative steps, effectively
stabilizing the optimization.</p></li>
</ul>
<p>In short, the proximal point framework interpolates between
fast-updating methods like gradient descent and slower, more implicit
schemes, depending on how quickly the reference point is advanced.</p>
</div>
<h4 id="proximal-policy-optimization-ppo">Proximal Policy Optimization
(PPO)</h4>
<p>Applying the proximal point method and using importance sampling for
reward estimation, we have <span class="math display">\[\theta^{k+1} =
\operatorname{arg\,max}_{\theta} \left\{ \mathbb{E}_{\mathcal D} \left[
\frac{\pi^\theta(a|s)}{\pi^{\mathtt{data}}(a|s)}
r(a,s) \right] - \beta D(\pi^\theta~||~ \pi^{\theta^{k}}) \right\}
.\]</span> We discuss two key design choices:</p>
<p><span class="math inline">\(\bullet\)</span> <em>Penalty
Function.</em> Typically, the discrepancy <span
class="math inline">\(D\)</span> is taken to be the KL divergence: <span
class="math display">\[D(\pi^\theta~||~ \pi^{\theta^{k}})
=  
\mathbb{E}_{s \sim p_0}[\mathrm{KL}(\pi^\theta(\cdot | s)~||~ \pi^{\theta^{k}}(\cdot |
s))].\]</span> However, various penalty functions have been used. We
review these in the sequel.</p>
<p><span class="math inline">\(\bullet\)</span> <em>Clipping
Function.</em> Perhaps the most important trick in PPO is the clip
function: <span class="math display">\[\begin{align*}
\theta^{k+1} = \operatorname{arg\,max}_{\theta} \left\{ \mathbb{E}_{\mathcal D} \left[
\mathcal C(w_\theta(a,s),
r(a,s)) \right] - \beta D(\pi^\theta~||~ \pi^{\theta^{k}}) \right\} ,
\end{align*}\]</span> where <span class="math inline">\(w_\theta(a,s) =
\frac{\pi^\theta(a|s)}{\pi^{\mathtt{data}}(a|s)},\)</span> and <span
class="math inline">\(\mathcal C\)</span> is defined as <span
class="math display">\[\mathcal C(w, r) = \min(w r, ~ \mathtt{clip}(w,
[1-\epsilon, 1+\epsilon]) \, r ).\]</span></p>
<p>In the following, we discuss the implications of the design choices
for the penalty and clip functions, respectively.</p>

    
    {{DISCUSSION_FORUM}}
</body>
</html>
