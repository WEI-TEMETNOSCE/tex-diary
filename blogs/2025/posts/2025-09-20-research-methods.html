<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>2025-09-20-research-methods</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="post.css" />
  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
<p><a href="run:2025-09-20-research-methods.tex">September 20</a></p>
<h1 id="research-methodology-and-experimental-design">Research
Methodology and Experimental Design</h1>
<p>Reviewed best practices for conducting machine learning research and
experimental validation.</p>
<h2 id="experimental-design-principles">Experimental Design
Principles</h2>
<ul>
<li><p><strong>Hypothesis Formation</strong>: Clear, testable statements
about expected outcomes</p></li>
<li><p><strong>Control Variables</strong>: Identify and control
confounding factors</p></li>
<li><p><strong>Sample Size</strong>: Use power analysis to determine
adequate sample sizes</p></li>
<li><p><strong>Randomization</strong>: Essential for causal
inference</p></li>
</ul>
<h2 id="statistical-testing">Statistical Testing</h2>
<p>For comparing model performance, consider:</p>
<p><strong>Paired t-test</strong> for comparing two models: <span
class="math display">\[t = \frac{\bar{d}}{s_d/\sqrt{n}}\]</span></p>
<p>where <span class="math inline">\(\bar{d}\)</span> is mean difference
and <span class="math inline">\(s_d\)</span> is standard deviation of
differences.</p>
<p><strong>McNemarâ€™s Test</strong> for classification accuracy: <span
class="math display">\[\chi^2 = \frac{(b - c)^2}{b + c}\]</span></p>
<p>where <span class="math inline">\(b\)</span> and <span
class="math inline">\(c\)</span> are counts of disagreements between
models.</p>
<h2 id="cross-validation-strategies">Cross-Validation Strategies</h2>
<ol>
<li><p><strong>k-fold CV</strong>: Standard approach, typically <span
class="math inline">\(k = 5\)</span> or <span class="math inline">\(k =
10\)</span></p></li>
<li><p><strong>Stratified CV</strong>: Maintains class distribution in
each fold</p></li>
<li><p><strong>Time Series CV</strong>: Forward chaining for temporal
data</p></li>
<li><p><strong>Leave-One-Out</strong>: When data is limited, but
computationally expensive</p></li>
</ol>
<h2 id="reproducibility-checklist">Reproducibility Checklist</h2>
<ul>
<li><p>Fix random seeds: <code>np.random.seed(42)</code></p></li>
<li><p>Document hyperparameters and model architectures</p></li>
<li><p>Version control datasets and preprocessing steps</p></li>
<li><p>Report confidence intervals, not just point estimates</p></li>
<li><p>Share code and trained models when possible</p></li>
</ul>
<p><strong>Statistical Significance</strong>: Use <span
class="math inline">\(p &lt; 0.05\)</span> but be aware of multiple
testing corrections.</p>
</body>
</html>
