<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>2025-09-20-machine-learning-demo</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="post.css" />
</head>
<body>
<p><a href="run:2025-09-20-machine-learning-demo.tex">September
20</a></p>
<h1 id="machine-learning-experiments">Machine Learning Experiments</h1>
<p>Today I conducted experiments with transformer models and achieved
significant improvements in performance. Here are the key findings and
visualizations.</p>
<h2 id="model-architecture">Model Architecture</h2>
<p>We implemented a custom transformer architecture as shown in Figure
<a href="#fig:architecture" data-reference-type="ref"
data-reference="fig:architecture">1</a>:</p>
<figure id="fig:architecture">
<img src="figures/model_architecture.svg" style="width:90.0%" />
<figcaption>Transformer model architecture with multi-head
attention</figcaption>
</figure>
<p>The architecture consists of:</p>
<ul>
<li><p>Input token embedding layer</p></li>
<li><p>Multi-head self-attention mechanism</p></li>
<li><p>Position-wise feed-forward networks</p></li>
<li><p>Output projection layer</p></li>
</ul>
<h2 id="training-results">Training Results</h2>
<p>The training process converged smoothly as demonstrated in Figure <a
href="#fig:loss" data-reference-type="ref"
data-reference="fig:loss">2</a>:</p>
<figure id="fig:loss">
<img src="figures/training_loss.svg" style="width:80.0%" />
<figcaption>Training loss curve showing convergence over 50
epochs</figcaption>
</figure>
<p>Key observations:</p>
<ol>
<li><p>Rapid initial convergence in first 10 epochs</p></li>
<li><p>Stable training without overfitting</p></li>
<li><p>Final loss: 0.032 (significant improvement)</p></li>
</ol>
<h2 id="performance-metrics">Performance Metrics</h2>
<p>Our model achieved the following results:</p>
<ul>
<li><p><strong>Accuracy</strong>: 94.7% on test set</p></li>
<li><p><strong>F1-Score</strong>: 0.943</p></li>
<li><p><strong>Training time</strong>: 2.3 hours on GPU</p></li>
</ul>
<p>This represents a 5.2% improvement over the baseline transformer
model.</p>
</body>
</html>
