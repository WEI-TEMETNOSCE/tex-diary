<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>2025-09-20-deep-learning</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="post.css" />
  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
<p><a href="run:2025-09-20-deep-learning.tex">September 20</a></p>
<h1 id="deep-learning-and-transformers">Deep Learning and
Transformers</h1>
<p>Todayâ€™s focus was on advanced deep learning architectures,
particularly the Transformer model.</p>
<h2 id="transformer-architecture">Transformer Architecture</h2>
<p>The Transformer <span class="citation"
data-cites="example2025"></span> revolutionized NLP with its attention
mechanism:</p>
<p><strong>Self-Attention Formula:</strong> <span
class="math display">\[\text{Attention}(Q, K, V) =
\text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(Q\)</span> = Queries matrix</p></li>
<li><p><span class="math inline">\(K\)</span> = Keys matrix</p></li>
<li><p><span class="math inline">\(V\)</span> = Values matrix</p></li>
<li><p><span class="math inline">\(d_k\)</span> = Dimension of key
vectors</p></li>
</ul>
<h2 id="multi-head-attention">Multi-Head Attention</h2>
<p>Instead of single attention, use <span
class="math inline">\(h\)</span> parallel attention heads: <span
class="math display">\[\text{MultiHead}(Q, K, V) =
\text{Concat}(\text{head}_1, ..., \text{head}_h)W^O\]</span></p>
<p>where <span class="math inline">\(\text{head}_i =
\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\)</span></p>
<h2 id="positional-encoding">Positional Encoding</h2>
<p>Since Transformers have no recurrence, positional information is
added: <span class="math display">\[PE_{(pos, 2i)} =
\sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)\]</span> <span
class="math display">\[PE_{(pos, 2i+1)} =
\cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)\]</span></p>
<h2 id="training-insights">Training Insights</h2>
<ol>
<li><p><strong>Warm-up</strong>: Learning rate increases linearly for
first steps</p></li>
<li><p><strong>Layer Normalization</strong>: Applied before each
sub-layer</p></li>
<li><p><strong>Residual Connections</strong>: Help with gradient flow in
deep networks</p></li>
<li><p><strong>Label Smoothing</strong>: Prevents overconfidence, <span
class="math inline">\(\epsilon = 0.1\)</span> typically</p></li>
</ol>
<p><strong>Key Advantage</strong>: Parallelizable training unlike RNNs,
enabling large-scale models like GPT and BERT.</p>
</body>
</html>
