<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>2025-09-20-neural-networks</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="post.css" />
  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
<p>&lt;TAGs&gt;: 2025, neural-networks, deep-learning, AI</p>
<p><a href="run:2025-09-20-neural-networks.tex">September 20</a></p>
<h1 id="neural-network-architecture-deep-dive">Neural Network
Architecture Deep Dive</h1>
<p>Explored the mathematical foundations of neural networks and
backpropagation algorithm.</p>
<h2 id="forward-propagation">Forward Propagation</h2>
<p>For a neural network with layers <span
class="math inline">\(L\)</span>, the forward pass computes: <span
class="math display">\[z^{[l]} = W^{[l]} a^{[l-1]} + b^{[l]}\]</span>
<span class="math display">\[a^{[l]} = g^{[l]}(z^{[l]})\]</span></p>
<p>where <span class="math inline">\(g^{[l]}\)</span> is the activation
function for layer <span class="math inline">\(l\)</span>.</p>
<h2 id="activation-functions">Activation Functions</h2>
<ul>
<li><p><strong>ReLU</strong>: <span class="math inline">\(g(z) = \max(0,
z)\)</span> - Most common, helps with vanishing gradients</p></li>
<li><p><strong>Sigmoid</strong>: <span class="math inline">\(g(z) =
\frac{1}{1 + e^{-z}}\)</span> - Output between 0 and 1</p></li>
<li><p><strong>Tanh</strong>: <span class="math inline">\(g(z) =
\frac{e^z - e^{-z}}{e^z + e^{-z}}\)</span> - Output between -1 and
1</p></li>
<li><p><strong>Leaky ReLU</strong>: <span class="math inline">\(g(z) =
\max(0.01z, z)\)</span> - Addresses dying ReLU problem</p></li>
</ul>
<h2 id="backpropagation">Backpropagation</h2>
<p>The key insight is using chain rule to compute gradients: <span
class="math display">\[\frac{\partial L}{\partial W^{[l]}} =
\frac{\partial L}{\partial z^{[l]}} \cdot \frac{\partial
z^{[l]}}{\partial W^{[l]}} = \delta^{[l]} \cdot
(a^{[l-1]})^T\]</span></p>
<h2 id="practical-implementation-notes">Practical Implementation
Notes</h2>
<ol>
<li><p>Initialize weights using Xavier/He initialization</p></li>
<li><p>Use batch normalization to stabilize training</p></li>
<li><p>Apply dropout for regularization: <span class="math inline">\(p =
0.5\)</span> typically works well</p></li>
<li><p>Learning rate scheduling: Start with <span
class="math inline">\(\alpha = 0.001\)</span>, decay by factor of
10</p></li>
</ol>
</body>
</html>
