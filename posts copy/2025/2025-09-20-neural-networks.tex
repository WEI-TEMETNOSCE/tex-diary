% Add your research tags below (comma-separated, case-insensitive)
% Year is automatically added as a tag
<TAGs>: 2025, neural-networks, deep-learning, AI

\documentclass[11pt,letterpaper]{article}

\newcommand{\workingDate}{\textsc{2025 $|$ September $|$ 20}}
\newcommand{\userName}{Research Student}
\newcommand{\institution}{University} 

\usepackage{diary_base}
\usepackage{diary_math}

\begin{document}
\href{run:2025-09-20-neural-networks.tex}{\Huge September 20} %##@@TitleDateString@@##

\section{Neural Network Architecture Deep Dive}

Explored the mathematical foundations of neural networks and backpropagation algorithm.

\subsection{Forward Propagation}
For a neural network with layers $L$, the forward pass computes:
$$z^{[l]} = W^{[l]} a^{[l-1]} + b^{[l]}$$
$$a^{[l]} = g^{[l]}(z^{[l]})$$

where $g^{[l]}$ is the activation function for layer $l$.

\subsection{Activation Functions}
\begin{itemize}
    \item \textbf{ReLU}: $g(z) = \max(0, z)$ - Most common, helps with vanishing gradients
    \item \textbf{Sigmoid}: $g(z) = \frac{1}{1 + e^{-z}}$ - Output between 0 and 1
    \item \textbf{Tanh}: $g(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$ - Output between -1 and 1
    \item \textbf{Leaky ReLU}: $g(z) = \max(0.01z, z)$ - Addresses dying ReLU problem
\end{itemize}

\subsection{Backpropagation}
The key insight is using chain rule to compute gradients:
$$\frac{\partial L}{\partial W^{[l]}} = \frac{\partial L}{\partial z^{[l]}} \cdot \frac{\partial z^{[l]}}{\partial W^{[l]}} = \delta^{[l]} \cdot (a^{[l-1]})^T$$

\subsection{Practical Implementation Notes}
\begin{enumerate}
    \item Initialize weights using Xavier/He initialization
    \item Use batch normalization to stabilize training
    \item Apply dropout for regularization: $p = 0.5$ typically works well
    \item Learning rate scheduling: Start with $\alpha = 0.001$, decay by factor of 10
\end{enumerate}

\bibliographystyle{apalike} 
\bibliography{reference}%##@@BibFileString@@##
\end{document}
