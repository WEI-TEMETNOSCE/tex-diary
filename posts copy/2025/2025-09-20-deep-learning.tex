% Add your research tags below (comma-separated, case-insensitive)
% Year is automatically added as a tag
%<TAGs>: 2025, deep-learning, neural-networks, transformers

\documentclass[11pt,letterpaper]{article}

\newcommand{\workingDate}{\textsc{2025 $|$ September $|$ 20}}
\newcommand{\userName}{Research Student}
\newcommand{\institution}{University} 

\usepackage{diary_base}
\usepackage{diary_math}

\begin{document}
\href{run:2025-09-20-deep-learning.tex}{\Huge September 20} %##@@TitleDateString@@##

\section{Deep Learning and Transformers}

Today's focus was on advanced deep learning architectures, particularly the Transformer model.

\subsection{Transformer Architecture}
The Transformer \cite{example2025} revolutionized NLP with its attention mechanism:

\textbf{Self-Attention Formula:}
$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

where:
\begin{itemize}
    \item $Q$ = Queries matrix
    \item $K$ = Keys matrix  
    \item $V$ = Values matrix
    \item $d_k$ = Dimension of key vectors
\end{itemize}

\subsection{Multi-Head Attention}
Instead of single attention, use $h$ parallel attention heads:
$$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O$$

where $\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$

\subsection{Positional Encoding}
Since Transformers have no recurrence, positional information is added:
$$PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$
$$PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$

\subsection{Training Insights}
\begin{enumerate}
    \item \textbf{Warm-up}: Learning rate increases linearly for first steps
    \item \textbf{Layer Normalization}: Applied before each sub-layer
    \item \textbf{Residual Connections}: Help with gradient flow in deep networks
    \item \textbf{Label Smoothing}: Prevents overconfidence, $\epsilon = 0.1$ typically
\end{enumerate}

\textbf{Key Advantage}: Parallelizable training unlike RNNs, enabling large-scale models like GPT and BERT.

\bibliographystyle{apalike} 
\bibliography{reference}%##@@BibFileString@@##
\end{document}
