% Add your research tags below (comma-separated, case-insensitive)
% Year is automatically added as a tag
<TAGs>: 2025, machine-learning, AI, algorithms

\documentclass[11pt,letterpaper]{article}

\newcommand{\workingDate}{\textsc{2025 $|$ September $|$ 20}}
\newcommand{\userName}{Research Student}
\newcommand{\institution}{University} 

\usepackage{diary_base}
\usepackage{diary_math}

\begin{document}
\href{run:2025-09-20-machine-learning.tex}{\Huge September 20} %##@@TitleDateString@@##

\section{Machine Learning Fundamentals Review}

Today I reviewed key machine learning concepts and algorithms. The main focus was on understanding the mathematical foundations.

\subsection{Supervised Learning}
\begin{itemize}
    \item \textbf{Linear Regression}: Minimizes the cost function $J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})^2$
    \item \textbf{Logistic Regression}: Uses sigmoid function $h_\theta(x) = \frac{1}{1 + e^{-\theta^T x}}$
    \item \textbf{Support Vector Machines}: Finds optimal hyperplane with maximum margin
\end{itemize}

\subsection{Key Insights}
The bias-variance tradeoff is fundamental to understanding model performance. High bias leads to underfitting, while high variance causes overfitting.

\textbf{Regularization techniques:}
\begin{enumerate}
    \item L1 regularization (Lasso): $\lambda \sum_{j=1}^{n} |\theta_j|$
    \item L2 regularization (Ridge): $\lambda \sum_{j=1}^{n} \theta_j^2$
    \item Elastic Net: Combines L1 and L2
\end{enumerate}

\subsection{Next Steps}
\begin{itemize}
    \item Implement gradient descent algorithm
    \item Study cross-validation techniques
    \item Explore ensemble methods
\end{itemize}

\bibliographystyle{apalike} 
\bibliography{reference}%##@@BibFileString@@##
\end{document}
