% Research Diary Collection Template for Research Student (University), 2025
\documentclass[letterpaper,11pt]{article}
\newcommand{\userName}{Research Student}
\newcommand{\institution}{University}
\newcommand{\diaryTitle}{Research Diary}
\newcommand{\workingDate}{2025 \diaryTitle}
% Modular package loading - choose what you need
% Files are symlinked in output directory for self-contained compilation

\usepackage{assets/styles/diary_base}        % Core packages and basic theorems
\usepackage{assets/styles/diary_ctheorems}   % Enhanced colored theorem environments
\usepackage{assets/styles/diary_commands}   % Personal shortcuts and commands

% Turn off showlabels for clean output
\renewcommand{\showlabels}[1]{}

\usepackage{tocloft}
\setlength{\cftbeforesecskip}{5pt}
\renewcommand*\contentsname{Research Student's Research Diary -- Contents}

\title{Research Diary Collection - 2025}
\author{Research Student}
\date{Compiled \today}

\begin{document}

\tableofcontents
\thispagestyle{empty}
\newpage


% Entry from 2025-09-28-2.tex
\href{run:2025-09-28-2.tex}{\Huge September 28} 

\section{Intro: Proximal Policy Optimization}
 %Gradient de
%Gradient descent is only one approach for optimization. 
%Gradient descent is 
%Generalizing

\paragraph{Proximal Point Method} 
Going beyond gradient descent, proximal point methods offer a more general framework for optimization. Starting from an initialization $\theta^0$, they perform iterative updates of the form:
$$
\theta^{k+1} = \argmax_{\theta} 
  \left \{ R(\theta) - \beta D(\theta, \theta^k) \right \},
$$
where, at iteration $k$, the next point $\theta^{k+1}$ is obtained by maximizing the objective while penalizing deviation from $\theta^k$. Here, $D(\theta, \theta')$ is a discrepancy or distance measure between $\theta$ and $\theta'$. The default choice is $D(\theta, \theta') = \frac{1}{2}\norm{\theta - \theta'}^2$, though it is common to use KL divergence in policy optimization as we show below. 

Each update from $\theta^k$ to $\theta^{k+1}$ requires solving an optimization problem, making the proximal point method a double-loop algorithm. Since this inner problem is often intractable, it is typically approximated using practical methods such as gradient descent or proximal gradient descent. 

Different approximations lead to different algorithmic variants.




\paragraph{Proximal Gradient Descent} 
Applying a first-order Taylor approximation to the objective function $R(\theta)$ gives:
$$
\theta^{k+1} = \argmax_{\theta} 
  \left \{ \dd R(\theta^k) \tt (\theta - \theta^k) - \beta D(\theta, \theta^k) \right \}.
$$
This yields the \emph{proximal gradient descent} method.

\paragraph{Preconditioned Gradient Descent}
Consider a quadratic proximal penalty:
$$
D(\theta, \theta^k) = \frac{1}{2}(\theta-\theta^k)\tt J(\theta^k)(\theta - \theta^k),
$$
where $J(\theta^k)$ is a positive definite preconditioning matrix chosen by the user. In this case, proximal gradient descent admits a closed-form solution:
$$
\theta^{k+1} = \theta^k + J(\theta^k)^{-1} \dd R(\theta^k).
$$
This is known as \emph{preconditioned gradient descent}.


\begin{remark}
Assume we use gradient descent to solve the inner loops of proximal methods. The difference from vanilla gradient descent then lies in how the \emph{reference point} is updated. To see this, let us consider the following generic update rule:
\[
\theta^{k+1} 
= \argmax_\theta \Bigl\{ \dd R(\theta^k)^\top (\theta - \theta^k) 
- \beta D(\theta, \theta^{\mathrm{ref},k}) \Bigr\},
\]
where $\theta^{\mathrm{ref},k}$ is the reference point used to anchor the update at iteration $k$.
%
The choice of $\theta^{\mathrm{ref},k}$ governs the behavior of the algorithm:
\begin{itemize}
\item If $\theta^{\mathrm{ref},k} = \theta^k$, the update reduces to \emph{proximal gradient descent}, where the reference point is updated as fast as the iterate itself.  

\item If the reference point is updated only once every $m$ iterations, 
i.e.\ $\theta^{\mathrm{ref},k} = \theta^{m\lfloor k/m \rfloor}$, 
then each outer iteration corresponds to running $m$ steps of a proximal 
gradient algorithm to approximately solve the proximal point update.

\item If $\theta^{\mathrm{ref},k}$ evolves more slowly, for example as an exponential moving average of past iterates, the algorithm takes more conservative steps, effectively stabilizing the optimization. 


\end{itemize}

In short, the proximal point framework interpolates between fast-updating methods like gradient descent and slower, more implicit schemes, depending on how quickly the reference point is advanced.
\end{remark}


% \begin{remark}

% The main difference between proximal point method and the vanilla gradient descent lies in how fast we update the reference point. Consider 
% $$
% \theta^{k+1} =\argmax_\theta  
%   \left \{ \dd R(\theta^k) \tt  (\theta - \theta^k) - \beta D(\theta, \theta^{\ttt{ref},k}) \right \}, 
% $$
% where $\theta^{\ttt{ref},k}$ can be in general an EMA update (slow). 
% And $\theta^{\ttt{ref}, k} = \theta^k$ yields proximal gradient descent, 
% and $\theta^{\ttt{ref}, k} = \theta^{k-m}$ yields proximal point method with inner loops solved with $m$ steps of proximal gradient algorithm. 
% \end{remark} 


\paragraph{Proximal Policy Optimization (PPO)}

Applying the proximal point method and using importance sampling for reward estimation, we have
$$
\theta^{k+1} = \argmax_{\theta} \left\{ \E_{\mathcal D} \left[ 
\frac{\pi^\theta(a|s)}{\pi^{\mathtt{data}}(a|s)}
r(a,s) \right] - \beta D(\pi^\theta~||~ \pi^{\theta^{k}}) \right\} .
$$
We discuss two key design choices: 

\noindent$\bullet$ \emph{Penalty Function.}  
Typically, the discrepancy $D$ is taken to be the KL divergence:
$$
D(\pi^\theta~||~ \pi^{\theta^{k}})
=  
\E_{s \sim p_0}[\KL(\pi^\theta(\cdot | s)~||~ \pi^{\theta^{k}}(\cdot | s))].
$$
However, various penalty functions have been used. We review these in the sequel.

\noindent$\bullet$ \emph{Clipping Function.}  
Perhaps the most important trick in PPO is the clip function:
\bb 
\theta^{k+1} = \argmax_{\theta} \left\{ \E_{\mathcal D} \left[ 
\mathcal C(w_\theta(a,s), 
r(a,s)) \right] - \beta D(\pi^\theta~||~ \pi^{\theta^{k}}) \right\} ,
\ee 
where 
$
w_\theta(a,s) = \frac{\pi^\theta(a|s)}{\pi^{\mathtt{data}}(a|s)},
$
and $\mathcal C$ is defined as
$$
\mathcal C(w, r) = \min(w r, ~ \mathtt{clip}(w, [1-\epsilon, 1+\epsilon]) \, r ).
$$

In the following, we discuss the implications of the design choices for the penalty and clip functions, respectively.





\clearpage


% Entry from 2025-09-28.tex
\href{run:2025-09-28.tex}{\Huge September 28} 

\section{Intro: Policy Gradient} 


For simplicity, let us first 
consider a one-step decision-making process (also known as a contextual bandit).  




\paragraph{Problem Setup}    
Let $s \in \set S$ be the state vector of the environment, and let $a \in \set A$ be the action taken by the agent.  
The reward function is denoted by $r(a,s)$.  
%
The agent's behavior is characterized by a policy, which is conveniently represented as a conditional probability distribution,  
$\pi(a \mid s)$,  
specifying the probability of selecting each action $a$ given a state $s$.  
%
The objective is to find the optimal policy $\pi^*$ that maximizes the expected reward:  
\[
%R(\pi \mid s) 
R(\pi) = \E_{s\sim p_0}\E_{a\sim \pi(\cdot | s)} [r(a,s)] = \sum_{s,a} p_\pi(a, s) r(a,s),
\]
where $p_0$ denotes the distribution of states,
and we have $p_{\pi}(a,s)$ denotes the joint distribution of $(a,s)$ when policy $\pi$ is used: 
$$
p_\pi(a,s) = \pi(a\mid s) p_0(s). 
$$
Typically, $p_0$ is unknown to us, but observed through a collection of data $\{s\datai\}$ drawn from $p_0$. 


\definecolor{slate}{HTML}{3B556B}
\definecolor{leaf}{HTML}{4F9D69}
\definecolor{amber}{HTML}{D39A28}
\definecolor{coolgray}{HTML}{5B6770}
\usetikzlibrary{arrows.meta, positioning, shadows.blur}
\begin{center}
\begin{tikzpicture}[
  >=Latex,
  node distance=24mm and 28mm,
  every node/.style={font=\small},
  box/.style={
    draw,
    rounded corners=4pt,
    minimum width=15mm,
    minimum height=9mm,
    align=center,
    very thick
  },
  edg/.style={very thick, -{Latex[length=3mm]}, draw=coolgray},
  lab/.style={midway, sloped, above, fill=white, inner sep=1pt}
]

% Nodes in a triangle
\node[box, draw=slate,  fill=slate!10]  (state)  {State\\(\(s\))};
\node[box, draw=leaf,   fill=leaf!10, right=30mm of state] (reward) {Reward\\(\(r\))};
\node[box, draw=amber,  fill=amber!12, above right=15mm and 10mm of state] (action) {Action\\\(\;a\)};

% Edges
\draw[edg] (state) -- node[lab] {\(\pi(a\mid s)\)} (action);
\draw[edg] (action) -- node[lab] {} (reward);
\draw[edg] (state)  -- node[lab] {} (reward);

\end{tikzpicture}
\end{center}


\paragraph{Policy Gradient} 
In practice, the policy $\pi(a \mid s) = \pi_\theta(a \mid s)$ is parameterized by a function with parameter $\theta$. 
Assume the state-action space $\mathcal S \times \mathcal A$ is finite or accountable,
the gradient of the expected reward with  respect to $\theta$ is 
\begin{align} 
\nabla_\theta R(\blue{\pi_\theta}) 
& =  \nabla_\theta \left ( \sum_{s,a} p_0(s) \blue{\pi_\theta(a \mid s)} r(a, s)  \right ) \notag \\
& = \sum_{s,a} p_0(s) \blue{\nabla_\theta \pi_\theta(a \mid s)} r(a, s) \ant{moving $\nabla_\theta$ into sum.} \notag \\ 
& = \sum_{s,a} p_0(s) \blue{\pi_\theta(a \mid s)}\blue{ \frac{ \nabla_\theta \pi_\theta(a \mid s) }{\pi_\theta(a \mid s)} } r(a, s) \notag \\
& = \E_{\blue{p_{\pi_\theta}}} \left[ r(a, s) \blue{\nabla_\theta \log \pi_\theta(a \mid s)} \right], \ant{$p_{\pi_\theta}(a,s) = \pi_\theta(a|s)p_0(s)$}, \notag %\label{equ:pgrad0}
\end{align}
where the parts depending on $\theta$ are highlighted, and we use the log-derivative trick $\dd_\theta \log \pi = \nabla_\theta \pi_\theta / \pi_\theta$. This allows us to express the derivative of an expectation with respect to a distribution as the expectation of the reward weighted by $\dd_\theta \log \pi_\theta(a \mid s)$.






\paragraph{Reward Baseline}
 For any policy $\pi^\theta$, it is easy to prove the following identity: 
 $$
 \E_{a\sim \pi_\theta(\cdot|s)}[\dd_\theta \log \pi_\theta(a\mid s)] =0,~~~~~ \forall s,~\theta.  
 $$
 
Hence, we can generalize the gradient formula above to 
\begin{align} \label{equ:pcgrad2}
\nabla_\theta  R(\pi_\theta ) 
& = \E_{p_{\pi_\theta}}  \left [  (r(a,s) - \med{v(s)})  \blue{\nabla_\theta  \log  \pi_\theta (a\mid s)}  \right ],
\end{align}
where $v(s)$ is \emph{any} function that does not depend on the action $a$. 
The choice of $v$ does not alter the expectation of the formula, but a proper choice of $v$ can reduce the variance of the mean estimation given empirical observations. 


One common choice of baseline is $v(s) = \E_{a\sim \pi_\theta(\cdot|s)}[r(a,s)]$, in which case the difference $r(a,s) - v(s)$ is known as the \emph{advantage function}:
$$
A(a, s) = r(a,s) - v(s).
$$
A positive value $A(a,s) > 0$ (respectively, negative $<0$) indicates that the action $a$ performs above (respectively, below) the average. 

%In \eqref{equ:pcgrad2}, the sign of $A(a,s)$ determines whether the gradient direction aligns with or opposes the log-probability gradient $\dd\log\pi_\theta(a|s)$. Under gradient descent, the parameters are updated to increase $\log\pi_\theta(a|s)$ for actions with positive $A(a,s)$ and decrease it for those with negative $A(a,s)$.


\paragraph{On-Policy Estimation} 
In practice, given observations $\{s\datai\}$ drawn from $p_0$ and $a\datai \sim \pi^\theta(\cdot|s\datai)$ drawn from the ongoing policy $\pi^\theta$, 
the gradient can be estimated by 
\bbb \label{equ:onpolicyestimate}
\nabla_\theta  R(\pi_\theta ) 
 \approx \frac{1}{n}\sum_{i=1}^n  
A\datai \nabla_\theta  \log  \pi_\theta (a\datai \mid s\datai),~~~~~~ 
 A\datai \defeq  A(a\datai, s\datai). 
\eee  
The gradient ascent update, also known as REINFORCE, 
$$
\theta\gets \theta + \epsilon \nabla_\theta  R(\pi_\theta ).  
$$
Intuitively, the gradient $\nabla_\theta \log \pi_\theta (a\datai \mid s\datai)$ for data points $(a\datai, s\datai)$ with positive advantages $A\datai > 0$ is positively weighted, increasing the probability $\pi_\theta(a \mid s)$ during gradient ascent. Conversely, for data points with negative advantages, the gradient is negatively weighted, decreasing the probability.  


\paragraph{Connection to Weighted MLE}  
Assuming the data $\{a\datai, s\datai\}$ are fixed, the policy gradient $\dd_\theta R(\pi_\theta)$ coincides with the gradient of the $A\datai$-weighted log-likelihood function:  
$$
\ell(\pi_\theta) = \frac{1}{n} \sum_{i=1}^n A\datai \log \pi_\theta(a\datai \mid s\datai).
$$  
This aligns with the intuition of maximizing $\log \pi(a \mid s)$ for data points with large (and positive) $A\datai$, while minimizing the log-likelihood for negative $A\datai$.  

The key difference, however, is that the data $\{a\datai, s\datai\}$ are drawn from the policy $\pi_\theta$ and thus depend on the current parameter $\theta$. As $\theta$ is updated during policy optimization, the data must either be regenerated or reweighted to reflect changes in $\pi_\theta$. In contrast, maximum likelihood estimation assumes the data are fixed.  

Hence, policy optimization can be viewed as an adaptively weighted maximum likelihood estimation (MLE), where the weights ($A\datai$) are adjusted across iterations using newly generated data.

 

\paragraph{Off-Policy Estimation via Importance Sampling}  
The gradient estimation in \eqref{equ:onpolicyestimate} 
is \emph{on-policy}, 
because the actions $a$ must be drawn from the current policy $\pi_\theta(\cdot | s)$. 
This does not yield efficient use of data
as new actions must be rolled out once the policy is updated, 
and data from different 
policies cannot be used. 
In comparison, \emph{off-policy} methods 
allow us to leverage actions from different policies. 


One common approach to off-policy estimation is importance sampling. 
Assume that the data $\{a\datai, s\datai\}$ are drawn from a \emph{behavior policy} $\pi^{\mathtt{data}}$. 
Using importance sampling, we have 
 
\bb 
\nabla_\theta R(\pi_\theta)  
& = \E_{p_{\pi^{\texttt{data}}}} \left[\frac{\pi_\theta(a|s)}{\pi^{\mathtt{data}}(a|s)} A(a,s) \nabla_\theta \log \pi_\theta (a \mid s) \right] \\ 
& \approx \frac{1}{z} \sum_{i=1}^n 
w\datai A\datai  \nabla_\theta \log \pi_\theta (a\datai \mid s\datai), ~~~~~ 
\ee 
where $w\datai$ is the importance weight: 
$$
w\datai = \frac{\pi_\theta(a\datai|s\datai)}{\pi^{\mathtt{data}}(a\datai|s\datai)}, 
$$
and the normalization constant $z$ is often taken as $z = \sum_i w\datai$. 

\begin{cremark}
In RL, \emph{on-policy methods} update the policy using data collected from the current policy itself.  
\emph{Off-policy methods} learn from data collected under a different policy, allowing reuse of past experiences and improving sample efficiency, but at the cost of a distribution mismatch between the behavior policy $\pi^{\text{data}}$ and the target policy $\pi_\theta$.  
\emph{Offline RL} is the case of off-policy learning where training relies entirely on a fixed dataset with no further interaction.  
\end{cremark}

%To correct for this mismatch, we use **importance sampling (IS)**, which reweights samples to account for the difference between $\pi_\theta$ and $\pi^{\text{data}}$. This allows off-policy learning while maintaining unbiased gradient estimates. However, naive importance sampling can suffer from high variance, necessitating variance reduction techniques.  

%In REINFORCE, we use simple gradient descent, but it does not make efficient use of data since it follows an on-policy approach, discarding past experiences after each update. Off-policy methods, combined with IS, help address this inefficiency by enabling data reuse.  



\clearpage


\bibliographystyle{apalike}
\bibliography{assets/bib/reference}


\end{document}
